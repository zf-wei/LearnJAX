{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import jax\n",
    "\n",
    "jax.config.update('jax_num_cpu_devices', 12)"
   ],
   "id": "cfd265f3a6b87b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "jax.devices()",
   "id": "c5e18713ac86d726",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "arr = jnp.arange(32.0).reshape(4, 8)\n",
    "arr.devices()"
   ],
   "id": "cb5fa1eafd7dd05",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "arr.sharding",
   "id": "d0d10a6a83d1bc24",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# pip install rich\n",
    "\n",
    "jax.debug.visualize_array_sharding(arr)"
   ],
   "id": "7c1dd675518e75d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax.sharding import PartitionSpec as P\n",
    "\n",
    "mesh = jax.make_mesh((2, 4), ('x', 'y')) # x和y是两个维度的名字\n",
    "sharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n",
    "print(sharding)\n",
    "\n",
    "# jax.sharding.NamedSharding 也可以直接写成 jax.NamedSharding"
   ],
   "id": "585b3789f58bdd44",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "arr_sharded = jax.device_put(arr, sharding)\n",
    "# 注意 这儿的 sharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n",
    "\n",
    "print(arr_sharded)\n",
    "jax.debug.visualize_array_sharding(arr_sharded)"
   ],
   "id": "9bc04b65fdb9c421",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Automatic parallelism via jit",
   "id": "85db3be7a883f98c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def f_elementwise(x):\n",
    "  return 2 * jnp.sin(x) + 1\n",
    "\n",
    "result = f_elementwise(arr_sharded)\n",
    "\n",
    "print(\"shardings match:\", result.sharding == arr_sharded.sharding)"
   ],
   "id": "7c4abadd494de6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(result)\n",
    "jax.debug.visualize_array_sharding(result)"
   ],
   "id": "1b4bc5c6b1c79e7b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def f_contract(x):\n",
    "  return x.sum(axis=0)\n",
    "\n",
    "result = f_contract(arr_sharded)\n",
    "jax.debug.visualize_array_sharding(result)\n",
    "print(result)"
   ],
   "id": "582253b4ef13a1d7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "ChatGPT告诉我\n",
    "\n",
    "为什么 visualize_array_sharding(result) 会显示 CPU 0,4？\n",
    "这是因为：\n",
    "\n",
    "每个 y 列对应多个 x 行的设备（如 x=0 和 x=1）\n",
    "\n",
    "当你从 P('x', 'y') 变为 P('y')，JAX 会默认让 x 方向上的两个设备共享该 shard\n",
    "\n",
    "所以结果 shard 在 mesh 的每列上的所有设备中“共享”，比如：\n",
    "\n",
    "(x=0,y=0) 和 (x=1,y=0) 都持有第一个结果 shard → CPU 0,4\n",
    "\n",
    "这是一种 sharding 与 replication 混合模式，用于确保后续计算中一致性和设备可用性。"
   ],
   "id": "d1a17f3a3d0124cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explicit sharding",
   "id": "fe1ca7051e60012a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "some_array = np.arange(8)\n",
    "print(f\"JAX-level type of some_array: {jax.typeof(some_array)}\")"
   ],
   "id": "80c55fb9ecffe2f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def foo(x):\n",
    "  print(f\"JAX-level type of x during tracing: {jax.typeof(x)}\")\n",
    "  return x + x\n",
    "\n",
    "foo(some_array)"
   ],
   "id": "a6f44a5edb45996",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax.sharding import AxisType\n",
    "\n",
    "mesh = jax.make_mesh((2, 4), (\"X\", \"Y\"),\n",
    "                     axis_types=(AxisType.Explicit, AxisType.Explicit))\n",
    "# 这里 AxisType.Explicit 是指我们在 shard 的时候，会指定怎么进行分配"
   ],
   "id": "a08b395f99235acf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "replicated_array = np.arange(8).reshape(4, 2)\n",
    "sharded_array = jax.device_put(replicated_array, jax.NamedSharding(mesh, P(\"X\", None)))\n",
    "\n",
    "print(f\"replicated_array type: {jax.typeof(replicated_array)}\")\n",
    "print(f\"sharded_array type: {jax.typeof(sharded_array)}\")\n",
    "print(sharded_array)"
   ],
   "id": "ad71042c13af899f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We should read the type f32[4@X, 2] as “a 4-by-2 array of 32-bit floats whose first dimension is sharded along mesh axis ‘X’. The array is replicated along all other mesh axes”",
   "id": "25d63e7cf2220e22"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "jax.debug.visualize_array_sharding(sharded_array)",
   "id": "ce7a09980d97e734",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "arg0 = jax.device_put(np.arange(4).reshape(4, 1),\n",
    "                      jax.NamedSharding(mesh, P(\"X\", None)))\n",
    "arg1 = jax.device_put(np.arange(8).reshape(1, 8),\n",
    "                      jax.NamedSharding(mesh, P(None, \"Y\")))\n",
    "\n",
    "@jax.jit\n",
    "def add_arrays(x, y):\n",
    "  ans = x + y # 这儿会自动广播\n",
    "  print(f\"x sharding: {jax.typeof(x)}\")\n",
    "  print(f\"y sharding: {jax.typeof(y)}\")\n",
    "  print(f\"ans sharding: {jax.typeof(ans)}\")\n",
    "  return ans\n",
    "\n",
    "with jax.sharding.use_mesh(mesh):\n",
    "  temp = add_arrays(arg0, arg1)\n",
    "  jax.debug.visualize_array_sharding(temp)"
   ],
   "id": "d9b2fedeb4b5075c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "add_arrays(arg0, arg1)\n",
    "\n",
    "print(f\"整个的求和结果是 {temp}\")\n",
    "\n",
    "print(\"试着查看 CPU0 上的数据\")\n",
    "for s in temp.addressable_shards:\n",
    "    if s.device.id == 0:  # 只看 CPU 0\n",
    "        print(f\"Data on CPU 0:\\n{s.data}\")"
   ],
   "id": "63c7797c90a045d8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Manual parallelism with shard_map",
   "id": "f07751dcc02cae45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import Mesh, PartitionSpec as P\n",
    "from jax.experimental.shard_map import shard_map # Correct import for shard_map\n",
    "\n",
    "# Assume f_elementwise is a function you have defined, for example:\n",
    "def f_elementwise(x):\n",
    "  return x * 2\n",
    "\n",
    "mesh = jax.make_mesh((8,), ('x',))\n",
    "\n",
    "f_elementwise_sharded = shard_map(\n",
    "    f_elementwise,\n",
    "    mesh=mesh,\n",
    "    in_specs=P('x'),\n",
    "    out_specs=P('x'))\n",
    "\n",
    "arr = jnp.arange(32)\n",
    "f_elementwise_sharded(arr)\n",
    "jax.debug.visualize_array_sharding(arr)"
   ],
   "id": "e1e9d360be5c865a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x = jnp.arange(32)\n",
    "print(f\"global shape: {x.shape=}\")\n",
    "\n",
    "def f(x):\n",
    "  print(f\"device local shape: {x.shape=}\")\n",
    "  return x * 2\n",
    "\n",
    "y = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)\n",
    "jax.debug.visualize_array_sharding(y)"
   ],
   "id": "f5ae7041553645a5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "  # print(f\"device local shape: {x.shape=}\")\n",
    "  sum_in_shard = x.sum(keepdims=True) # 如果没有这个 keepdims=True，sum_in_shard 就是一个标量 程序会报错\n",
    "  #print(f\"sum_in_shard shape: {sum_in_shard.shape=}\")\n",
    "  return sum_in_shard\n",
    "  #return jnp.sum(x, keepdims=True)\n",
    "\n",
    "#print(x)\n",
    "x_sharded = jax.device_put(x, jax.NamedSharding(mesh, P(\"x\",))\n",
    "                           )\n",
    "for s in x_sharded.addressable_shards:\n",
    "    print(f\"Data on {s.device}: {s.data}\")\n",
    "\n",
    "jax.debug.visualize_array_sharding(x_sharded)\n",
    "z = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)\n",
    "print(z)\n",
    "jax.debug.visualize_array_sharding(z)"
   ],
   "id": "5a3fe2986b1cdedd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def g(x):\n",
    "  sum_in_shard = x.sum(keepdims=True) # 如果没有这个 keepdims=True，sum_in_shard 就是一个标量 这个地方不会报错、\n",
    "  #print(f\"sum_in_shard shape: {sum_in_shard.shape=}\")\n",
    "  return jax.lax.psum(sum_in_shard, 'x')\n",
    "\n",
    "print(shard_map(g, mesh=mesh, in_specs=P('x'), out_specs=P())(x))\n",
    "# 注意，要跨 partition 进行计算，需要使用 jax.lax.psum，而且 out_specs 也要设置为 P() 这样才是只有一个输出数字嘛\n",
    "# 如果设置为 P('x')，尝试下面的代码\n",
    "print(shard_map(g, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x))"
   ],
   "id": "837faa8d5ec8847d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Comparing the three approaches",
   "id": "804bdaeaa022062e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@jax.jit\n",
    "def layer(x, weights, bias):\n",
    "  return jax.nn.sigmoid(x @ weights + bias)"
   ],
   "id": "9a57e21d8c206f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "x = rng.normal(size=(32,))\n",
    "weights = rng.normal(size=(32, 4))\n",
    "bias = rng.normal(size=(4,))\n",
    "\n",
    "x = jnp.array(x)\n",
    "weights = jnp.array(weights)\n",
    "bias = jnp.array(bias)\n",
    "\n",
    "layer(x, weights, bias)"
   ],
   "id": "cb798afffda1b06a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "mesh = jax.make_mesh((8,), ('x',))\n",
    "x_sharded = jax.device_put(x, jax.NamedSharding(mesh, P('x')))\n",
    "weights_sharded = jax.device_put(weights, jax.NamedSharding(mesh, P()))\n",
    "\n",
    "print(layer(x_sharded, weights_sharded, bias))\n",
    "\n",
    "jax.debug.visualize_array_sharding(x_sharded)\n",
    "jax.debug.visualize_array_sharding(weights_sharded)"
   ],
   "id": "b11afa3a23e8113d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "explicit_mesh = jax.make_mesh((8,), ('X',), axis_types=(AxisType.Explicit,))\n",
    "\n",
    "x_sharded = jax.device_put(x, jax.NamedSharding(explicit_mesh, P('X')))\n",
    "jax.debug.visualize_array_sharding(x_sharded)\n",
    "weights_sharded = jax.device_put(weights, jax.NamedSharding(explicit_mesh, P()))\n",
    "\n",
    "@jax.jit\n",
    "def layer_auto(x, weights, bias):\n",
    "  print(f\"x sharding: {jax.typeof(x)}\")\n",
    "  print(f\"weights sharding: {jax.typeof(weights)}\")\n",
    "  print(f\"bias sharding: {jax.typeof(bias)}\")\n",
    "  out = layer(x, weights, bias)\n",
    "  print(f\"out sharding: {jax.typeof(out)}\")\n",
    "  return out\n",
    "\n",
    "with jax.sharding.use_mesh(explicit_mesh):\n",
    "  layer_auto(x_sharded, weights_sharded, bias)"
   ],
   "id": "6a9f375c2996e313",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z = jnp.arange(32)\n",
    "explicit_mesh = jax.make_mesh((8,), ('X',), axis_types=(AxisType.Explicit,))\n",
    "z_sharded = jax.device_put(z, jax.NamedSharding(explicit_mesh, P('X')))\n",
    "print(f\"z sharding: {jax.typeof(z_sharded)}\")"
   ],
   "id": "d4249a52bef55ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "上一个单元格的输出中，变量 z 是一个 JAX 数组，它：\n",
    "- 数据类型为 int32 (32位整数)。\n",
    "- 有一个维度，其全局大小为 32。\n",
    "- 这个维度是沿着一个名为 X 的设备网格轴进行分片 (sharded) 的。\n",
    "\n",
    "简单来说，数组 z 是一个包含32个整数的数组，这些整数被分散存储在与网格轴 X 关联的多个设备上。"
   ],
   "id": "237a263157d57c67"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "@jax.jit\n",
    "@partial(shard_map, mesh=mesh,\n",
    "         in_specs=(P('x'), P('x', None), P(None)),\n",
    "         out_specs=P(None))\n",
    "def layer_sharded(x, weights, bias):\n",
    "  print(f\"x sharding: {jax.typeof(x)}\")\n",
    "  print(f\"weights sharding: {jax.typeof(weights)}\")\n",
    "  print(f\"bias sharding: {jax.typeof(bias)}\")\n",
    "  return jax.nn.sigmoid(jax.lax.psum(x @ weights, 'x') + bias)\n",
    "\n",
    "print(layer_sharded(x, weights, bias))\n",
    "\n",
    "print(\"注意到 x 的长度是32，而 mesh 的长度是8，所以 x 会被分成8份，每份的长度是4\")"
   ],
   "id": "b96ab99d1f3358a0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
