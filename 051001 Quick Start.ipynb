{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 导入必要的工具\n",
    "from jax import numpy as jnp\n",
    "from jax import random\n",
    "from jax import grad, jit, vmap\n",
    "from jax import jacfwd, jacrev\n",
    "from jax import jacobian\n",
    "from jax import hessian\n",
    "import numpy as np"
   ],
   "id": "fc9d8f7560c0cda7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 下面用一个简单的例子来说明 Jax 相对于 numpy 的性能优势",
   "id": "5a7b5372d9ee5c5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 用 Jax 写 selu 函数 并且在随机数上进行测试\n",
    "def jselu(x, alpha=1.67, lmbda=1.05):\n",
    "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
    "\n",
    "x = jnp.arange(5.0)\n",
    "print(jselu(x))\n",
    "\n",
    "key = random.PRNGKey(1701)\n",
    "x = random.normal(key, (1_000_000,))\n",
    "%timeit jselu(x).block_until_ready()"
   ],
   "id": "f8ade83d9e13153a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 用 numpy 写 selu 函数 并且在随机数上进行测试\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)\n",
    "\n",
    "x = np.arange(5.0)\n",
    "print(selu(x))\n",
    "\n",
    "x = np.random.normal(size=(1_000_000,))\n",
    "%timeit selu(x)"
   ],
   "id": "b086cbc369ca6b7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 用 jit 编译 jselu 函数, 性能更好了\n",
    "selu_jit = jit(jselu)\n",
    "_ = selu_jit(x)  # compiles on first call\n",
    "%timeit selu_jit(x).block_until_ready()"
   ],
   "id": "2f4f9a97a6b77b47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 下面是随机数和种子的学习",
   "id": "f947815232852b9c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 测试学习 numpy 中的全局和局部随机数生成器\n",
    "np.random.seed(2026)\n",
    "y = np.random.normal(size=2)\n",
    "print(y)\n",
    "\n",
    "rd = np.random.default_rng(3)\n",
    "x = rd.normal(size=2)\n",
    "print(x)\n",
    "\n",
    "z = np.random.uniform(size=3)\n",
    "print(z)"
   ],
   "id": "9d2fd836bdf0363c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 测试 Jax 中的随机数生成器并且学着用 key 和分裂操作\n",
    "\n",
    "# 创建主随机数密钥\n",
    "key = random.PRNGKey(1)\n",
    "print(key)\n",
    "\n",
    "# 分裂主密钥生成两个子密钥\n",
    "key, subkey = random.split(key)\n",
    "\n",
    "# 使用子密钥生成随机数\n",
    "x = random.normal(subkey, shape=(4,))\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "y = random.uniform(subkey, shape=(4,))\n",
    "\n",
    "print(\"正态分布随机数:\", x)\n",
    "print(\"均匀分布随机数:\", y)"
   ],
   "id": "212d7fdf5b945829",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 下面是 Jax 中的自动微分",
   "id": "466f1b39a99a760e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 自动微分的一个初步例子\n",
    "# grad 函数是梯度，可以有多维输入但是只能有一维输出\n",
    "def sum_logistic(x):\n",
    "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
    "\n",
    "x_small = jnp.arange(3.)\n",
    "derivative_fn = grad(sum_logistic)\n",
    "print(x_small, derivative_fn(x_small))"
   ],
   "id": "13954eeea50fd9c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# grad 和 git 可以随机嵌套使用\n",
    "# 这个单元格比较了是否用 jit 的不同性能\n",
    "\n",
    "test_fun = grad(jit(grad(jit(grad(sum_logistic)))))\n",
    "_ = test_fun(1.0)  # compiles on first call\n",
    "\n",
    "%timeit test_fun(1.0).block_until_ready()\n",
    "%timeit grad(grad(grad(sum_logistic)))(1.0).block_until_ready()"
   ],
   "id": "62d1de166840741f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 因为 grad 处理的函数只能有一维输出，所以我们只能使用 0.0 作为测试例子\n",
    "grad_exp = grad(jnp.exp)\n",
    "print(grad_exp(0.0))\n",
    "# print(grad_exp(0.0, 1.0))\n",
    "# 这个例子是错误的\n",
    "# 因为当喂进去一个二维向量，那么 jnp.exp 的输出也会使用一个二维向量，所以再嵌套grad就会报错\n",
    "\n",
    "\n",
    "# 下面是一个正确的例子\n",
    "# 作为替代，可以使用 vmap 来处理多维输入\n",
    "grad_exp = vmap(grad(jnp.exp))\n",
    "print(grad_exp(x_small))\n",
    "\n",
    "# 或者使用 jacobian 来处理多维输入\n",
    "# jacobian 处理的函数可以有多维输入和多维输出\n",
    "\n",
    "print(jacobian(jnp.exp)(x_small))\n",
    "# 或者继续取对角线得到和向量化一样的结果\n",
    "print(jnp.diag(jacobian(jnp.exp)(x_small)))"
   ],
   "id": "3ce247b4892b9eea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 尝试两种不同形式的 hessian\n",
    "def hessian1(fun):\n",
    "  return jit(jacfwd(jacrev(fun)))\n",
    "def hessian2(fun):\n",
    "  return jit(jacrev(jacfwd(fun)))\n",
    "\n",
    "_ = hessian1(jnp.exp)(x_small)  # compiles on first call\n",
    "_ = hessian2(jnp.exp)(x_small)  # compiles on first call"
   ],
   "id": "ca18c27ed3854aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 比较两种不同形式的自实现 hessian 以及 jax 中的 hessian\n",
    "%timeit hessian1(sum_logistic)(x_small).block_until_ready()\n",
    "%timeit hessian2(sum_logistic)(x_small).block_until_ready()\n",
    "\n",
    "# 下面是 jax 中的 hessian\n",
    "%timeit hessian(sum_logistic)(x_small).block_until_ready()\n",
    "\n",
    "# 可以看到 jax 提供的 hessian 的性能是最好的"
   ],
   "id": "116bc927a9376d0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 下面是 Jax 中的自动向量化",
   "id": "b6acd6f0813daa40"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "key = random.PRNGKey(2025)\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "mat = random.normal(subkey1, (150, 100))\n",
    "batched_x = random.normal(subkey2, (10, 100))\n",
    "\n",
    "def apply_matrix(x):\n",
    "  return jnp.dot(mat, x)"
   ],
   "id": "38ba5b7b4e452be5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def naively_batched_apply_matrix(v_batched):\n",
    "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
    "\n",
    "print(apply_matrix(batched_x[0]).shape)\n",
    "\n",
    "print('Naively batched')\n",
    "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
   ],
   "id": "d169b95643572c4f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@jit\n",
    "def vmap_batched_apply_matrix(batched_x):\n",
    "  return vmap(apply_matrix)(batched_x)\n",
    "\n",
    "_ = vmap_batched_apply_matrix(batched_x)  # compiles on first call\n",
    "\n",
    "np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
    "                           vmap_batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
    "print('Auto-vectorized with vmap')\n",
    "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
   ],
   "id": "e9661cc5ca7a6a24",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
