{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "key = random.key(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 预热：计算二阶导数\n",
    "\n",
    "def hessian(f):\n",
    "  return jax.jacfwd(jax.grad(f))\n",
    "\n",
    "def f(x):\n",
    "  return jnp.dot(x, x)\n",
    "\n",
    "hessian(f)(jnp.array([1., 2., 3.]))"
   ],
   "id": "ab58520a982f8636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Higher-order optimization",
   "id": "fbe5f718de5e30f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stopping gradients",
   "id": "2720df1c8fba9a9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Value function and initial parameters\n",
    "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
    "theta = jnp.array([0.1, -0.1, 0.])\n",
    "\n",
    "# An example transition.\n",
    "s_tm1 = jnp.array([1., 2., -1.])\n",
    "r_t = jnp.array(1.)\n",
    "s_t = jnp.array([2., 1., 0.])"
   ],
   "id": "d875436eac35ddf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def td_loss(theta, s_tm1, r_t, s_t):\n",
    "  v_tm1 = value_fn(theta, s_tm1)\n",
    "  target = r_t + value_fn(theta, s_t)\n",
    "  return -0.5 * ((target - v_tm1) ** 2)\n",
    "\n",
    "td_update = jax.grad(td_loss)\n",
    "delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
    "\n",
    "delta_theta"
   ],
   "id": "9936d44482ba10e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def td_loss(theta, s_tm1, r_t, s_t):\n",
    "  v_tm1 = value_fn(theta, s_tm1)\n",
    "  target = r_t + value_fn(theta, s_t)\n",
    "  return -0.5 * ((jax.lax.stop_gradient(target) - v_tm1) ** 2)\n",
    "# This is the most important line in the code.\n",
    "\n",
    "td_update = jax.grad(td_loss)\n",
    "delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
    "\n",
    "print(delta_theta)\n",
    "\n",
    "# 手动写代码验证上面的结果\n",
    "grad_fn = jax.grad(value_fn)\n",
    "(r_t+value_fn(theta, s_t)-value_fn(theta, s_tm1))*grad_fn(theta, s_tm1)"
   ],
   "id": "b74ba0bb6c1c403f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Straight-through estimator using stop_gradient",
   "id": "6b19864bca53767d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "  return jnp.round(x)  # non-differentiable\n",
    "\n",
    "def straight_through_f(x):\n",
    "  # Create an exactly-zero expression with Sterbenz lemma that has\n",
    "  # an exactly-one gradient.\n",
    "  zero = x - jax.lax.stop_gradient(x)\n",
    "  return zero + jax.lax.stop_gradient(f(x))\n",
    "\n",
    "print(\"f(x): \", f(3.2))\n",
    "print(\"straight_through_f(x):\", straight_through_f(3.2))\n",
    "\n",
    "print(\"grad(f)(x):\", jax.grad(f)(3.2))\n",
    "print(\"grad(straight_through_f)(x):\", jax.grad(straight_through_f)(3.2))\n",
    "\n",
    "# 前向传播就是函数求值\n",
    "# 反向传播就是函数求导（用链式法则）"
   ],
   "id": "e994785c9148900a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Per-example gradients",
   "id": "5b0046d0eb4a0603"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "perex_grads = jax.jit(jax.vmap(jax.grad(td_loss), in_axes=(None, 0, 0, 0)))\n",
    "\n",
    "# Test it:\n",
    "batched_s_tm1 = jnp.stack([s_tm1, s_tm1])\n",
    "batched_r_t = jnp.stack([r_t, r_t])\n",
    "batched_s_t = jnp.stack([s_t, s_t])\n",
    "\n",
    "perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)"
   ],
   "id": "1621a4ff12c82f1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hessian-vector products with jax.grad-of-jax.grad",
   "id": "c778fad8612e7930"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def hvp(f, x, v):\n",
    "    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
    "\n",
    "def gprod(f, x, v):\n",
    "    return grad(f)(x) @ v\n",
    "\n",
    "# 或者不用 lambda 用下面的这种写法？\n",
    "def hvp2(f, x, v):\n",
    "    return grad(gprod, argnums=1)(x) @ v"
   ],
   "id": "d446b7bf6deffa0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# jax.grad 是什么形状的？\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# 定义目标函数\n",
    "def f(x):\n",
    "    return jnp.sum(x ** 2)  # 简单的二次函数\n",
    "\n",
    "# 输入变量\n",
    "x = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# 计算梯度\n",
    "grad_f = grad(f)(x)\n",
    "\n",
    "print(\"输入变量 x 的形状:\", x.shape)\n",
    "print(\"梯度向量 grad_f 的形状:\", grad_f.shape)"
   ],
   "id": "a5b372820312c2fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 补课：点积和矩阵乘法\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# 一维向量点积（标量）\n",
    "a = jnp.array([1, 2, 3])\n",
    "b = jnp.array([4, 5, 6])\n",
    "print(\"一维向量点积（标量）\")\n",
    "print(jnp.dot(a, b))   # 32\n",
    "print(a @ b)    # 32\n",
    "print(jnp.matmul(a, b))  # [4 10 18]\n",
    "\n",
    "print(jnp.inner(a, b))  # 32\n",
    "print(jnp.vdot(a, b))   # 32\n",
    "\n",
    "# 二维矩阵乘法\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "print(jnp.dot(A, B))   # works\n",
    "print(A @ B)           # works\n",
    "print(jnp.matmul(A, B))   # works\n",
    "\n",
    "# 更高维度建议用 jnp.matmul 或 @\n",
    "# 不推荐用 jnp.dot\n",
    "\n",
    "# jnp.tensordot 这个比较复杂，有待进一步学习"
   ],
   "id": "c1061b41522af656",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Jacobians and Hessians using jax.jacfwd and jax.jacrev",
   "id": "def00cb5ccbb0616"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "# Define a sigmoid function.\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Outputs probability of a label being true.\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "\n",
    "# Initialize random model coefficients\n",
    "key, W_key, b_key = random.split(key, 3)\n",
    "W = random.normal(W_key, (3,))\n",
    "b = random.normal(b_key, ())\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "J = jacfwd(f)(W)\n",
    "print(\"jacfwd result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "J = jacrev(f)(W)\n",
    "print(\"jacrev result, with shape\", J.shape)\n",
    "print(J)"
   ],
   "id": "3cc778fb83ba272f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These two functions compute the same values (up to machine numerics), but differ in their implementation: jax.jacfwd() uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices (more outputs than inputs), while jax.jacrev() uses reverse-mode, which is more efficient for “wide” Jacobian matrices (more inputs than outputs). For matrices that are near-square, jax.jacfwd() probably has an edge over jax.jacrev().",
   "id": "6481dfa457b9688e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_dict(params, inputs):\n",
    "    return predict(params['W'], params['b'], inputs)\n",
    "\n",
    "J_dict_fwd = jacfwd(predict_dict)({'W': W, 'b': b}, inputs)\n",
    "J_dict_rev = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n",
    "\n",
    "for k, v in J_dict_fwd.items():\n",
    "    print(f\"Jacobian from {k} to logits is\")\n",
    "    print(v)\n",
    "for k, v in J_dict_rev.items():\n",
    "    print(f\"Jacobian from {k} to logits is\")\n",
    "    print(v)"
   ],
   "id": "ac2dfdbb8bc679f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# How it’s made: Two foundational autodiff functions",
   "id": "a23ed7af13655e3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Jacobian-Vector products (JVPs, a.k.a. forward-mode autodiff)",
   "id": "50c6dfad756e9cfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import jvp\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "v = random.normal(subkey, W.shape)\n",
    "\n",
    "# Push forward the vector `v` along `f` evaluated at `W`\n",
    "y, u = jvp(f, (W,), (v,))\n",
    "print(\"使用 jvp 计算的结果\")\n",
    "print(\"y:\", y)\n",
    "print(\"u:\", u)\n",
    "\n",
    "print(\"下面是手动验证的代码\")\n",
    "print(f\"y: {f(W)}\\nu: {jax.jacobian(f)(W)@v}\")\n",
    "\n",
    "# 教程中的 “FLOP” 是 Floating Point Operation（浮点运算） 的缩写，是衡量计算量的一个基本单位。"
   ],
   "id": "529c7006cf2b3a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vector-Jacobian products (VJPs, a.k.a. reverse-mode autodiff)",
   "id": "d915ed078ccadcb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import vjp\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "y, vjp_fun = vjp(f, W)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "#u = random.normal(subkey, y.shape)\n",
    "u = jnp.array([1., 0., 0., 0.])  # 只取第一个元素的梯度\n",
    "\n",
    "# Pull back the covector `u` along `f` evaluated at `W`\n",
    "v = vjp_fun(u)\n",
    "\n",
    "u, v"
   ],
   "id": "5f8fb7e54d60dedf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vector-valued gradients with VJPs",
   "id": "d55d63f346829e15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vgrad(f, x):\n",
    "  y, vjp_fn = vjp(f, x)\n",
    "  return vjp_fn(jnp.ones(y.shape))[0]\n",
    "\n",
    "x = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "print(vgrad(lambda x: 3*x**2, x))"
   ],
   "id": "71dd161d776cf612",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 或者也可以手动实现\n",
    "\n",
    "f = lambda x:3*x**2\n",
    "vdf = jax.vmap(grad(f))\n",
    "vdf = vdf(x.reshape(-1))\n",
    "print(vdf, \"但是这个形状变了！\")\n",
    "\n",
    "varf = lambda x: jnp.sum(3 * x ** 2)\n",
    "print(varf(x), \"这个函数变了！\")\n",
    "vardf = jax.grad(varf)\n",
    "print(vardf(x))  # 输出 shape 为 (2,2)，结果是全 6"
   ],
   "id": "62f7846e1417e5c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hessian-vector products using both forward- and reverse-mode",
   "id": "88407a1365e9935"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def f(X):\n",
    "  return jnp.sum(jnp.tanh(X)**2)\n",
    "\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "X = random.normal(subkey1, (30, 40))\n",
    "V = random.normal(subkey2, (30, 40))\n",
    "\n",
    "# forward-over-reverse\n",
    "def hvp(f, primals, tangents):\n",
    "  return jvp(grad(f), primals, tangents)[1]\n",
    "\n",
    "# Reverse-over-forward\n",
    "def hvp_revfwd(f, primals, tangents):\n",
    "  g = lambda primals: jvp(f, primals, tangents)[1]\n",
    "  return grad(g)(primals)\n",
    "\n",
    "# Reverse-over-reverse, only works for single arguments\n",
    "def hvp_revrev(f, primals, tangents):\n",
    "  x, = primals\n",
    "  v, = tangents\n",
    "  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
    "\n",
    "print(\"Forward over reverse\")\n",
    "%timeit -n10 -r3 hvp(f, (X,), (V,))\n",
    "print(\"Reverse over forward\")\n",
    "%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))\n",
    "print(\"Reverse over reverse\")\n",
    "%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))\n",
    "print(\"Naive full Hessian materialization\")\n",
    "%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)"
   ],
   "id": "2b7d115173b37ce1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Composing VJPs, JVPs, and jax.vmap",
   "id": "716126a4544f68eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Jacobian-Matrix and Matrix-Jacobian products",
   "id": "4880b054a351884f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "# Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.\n",
    "# First, use a list comprehension to loop over rows in the matrix M.\n",
    "def loop_mjp(f, x, M):\n",
    "    y, vjp_fun = vjp(f, x)\n",
    "    return jnp.vstack([vjp_fun(mi) for mi in M])\n",
    "\n",
    "# Now, use vmap to build a computation that does a single fast matrix-matrix\n",
    "# multiply, rather than an outer loop over vector-matrix multiplies.\n",
    "def vmap_mjp(f, x, M):\n",
    "    y, vjp_fun = vjp(f, x)\n",
    "    outs, = vmap(vjp_fun)(M)\n",
    "    return outs\n",
    "\n",
    "def vmap_mjp2(f, x, M):\n",
    "    y, jvp_fun = vmap(jvp, in_axes=(None, None, 0))(f, (x,), (M,))\n",
    "    outs = jvp_fun\n",
    "    return outs\n",
    "\n",
    "key = random.key(0)\n",
    "num_covecs = 128\n",
    "U = random.normal(key, (num_covecs,) + y.shape)\n",
    "V = random.normal(key, (num_covecs,) + W.shape)\n",
    "\n",
    "\n",
    "loop_vs = loop_mjp(f, W, M=U)\n",
    "print('Non-vmapped Matrix-Jacobian product')\n",
    "%timeit -n10 -r3 loop_mjp(f, W, M=U)\n",
    "\n",
    "print('\\nVmapped Matrix-Jacobian product, reverse-mode')\n",
    "vmap_vs = vmap_mjp(f, W, M=U)\n",
    "%timeit -n10 -r3 vmap_mjp(f, W, M=U)\n",
    "\n",
    "print('\\nVmapped Matrix-Jacobian product forward-mode')\n",
    "vmap_vs2 = vmap_mjp2(f, W, M=V)\n",
    "%timeit -n10 -r3 vmap_mjp2(f, W, M=V)\n",
    "\n",
    "# assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical'"
   ],
   "id": "d2be2fd054ff571",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The implementation of jax.jacfwd and jax.jacrev",
   "id": "e5581e737b0ffcf3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import jacrev as builtin_jacrev\n",
    "\n",
    "def our_jacrev(f):\n",
    "    def jacfun(x):\n",
    "        y, vjp_fun = vjp(f, x)\n",
    "        # Use vmap to do a matrix-Jacobian product.\n",
    "        # Here, the matrix is the Euclidean basis, so we get all\n",
    "        # entries in the Jacobian at once.\n",
    "        J, = vmap(vjp_fun, in_axes=0)(jnp.eye(len(y)))\n",
    "        return J\n",
    "    return jacfun\n",
    "\n",
    "assert jnp.allclose(builtin_jacrev(f)(W), our_jacrev(f)(W)), 'Incorrect reverse-mode Jacobian results!'"
   ],
   "id": "5e7a7c85a228dd53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import jacfwd as builtin_jacfwd\n",
    "\n",
    "def our_jacfwd(f):\n",
    "    def jacfun(x):\n",
    "        _jvp = lambda s: jvp(f, (x,), (s,))[1]\n",
    "        Jt = vmap(_jvp, in_axes=1)(jnp.eye(len(x)))\n",
    "        return jnp.transpose(Jt)\n",
    "    return jacfun\n",
    "\n",
    "assert jnp.allclose(builtin_jacfwd(f)(W), our_jacfwd(f)(W)), 'Incorrect forward-mode Jacobian results!'"
   ],
   "id": "636575db07ef9660",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 补课：闭包与函数嵌套调用\n",
    "\n",
    "def func1(a):\n",
    "    def func2(b):\n",
    "        return a + b\n",
    "    return func2\n",
    "\n",
    "func1(1)(2)  # 3"
   ],
   "id": "71a06fc636c0a358",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "    try:\n",
    "        if x < 3.:\n",
    "            return 2 * x ** 3\n",
    "        else:\n",
    "            raise ValueError\n",
    "    except ValueError:\n",
    "        return jnp.pi * x\n",
    "\n",
    "y, f_vjp = vjp(f, -1.) # This is OK with JIT, since the second parameter is already given.\n",
    "print(jit(f_vjp)(1.))"
   ],
   "id": "ab1bc8bc7f2684e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Custom derivative rules for JAX-transformable Python functions",
   "id": "156d19f8c2c71b7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "✅ 何时用 @custom_jvp？\n",
    "- 函数里包含 非平滑或不可导操作（如 round, clip, quantize 等）；\n",
    "- 或者你知道函数的导数形式比自动推导更稳定/高效/符合你的预期。"
   ],
   "id": "dcc4c5e51a8705d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import custom_jvp\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "@custom_jvp\n",
    "def f(x, y):\n",
    "  return jnp.sin(x) * y\n",
    "\n",
    "@f.defjvp\n",
    "def f_jvp(primals, tangents):\n",
    "  #print(\"primals:\", primals, type(primals))\n",
    "  x, y = primals\n",
    "  x_dot, y_dot = tangents\n",
    "  primal_out = f(x, y)\n",
    "  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n",
    "  return primal_out, tangent_out\n",
    "\n",
    "print(f(2., 3.))\n",
    "y, y_dot = jvp(f, (2., 3.), (1., 0.))\n",
    "print(y)\n",
    "print(y_dot)\n",
    "print(grad(f)(2., 3.))"
   ],
   "id": "c358361b7cfe911a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Equivalent alternative using the `defjvps` convenience wrapper\n",
    "\n",
    "@custom_jvp\n",
    "def f(x, y):\n",
    "  return jnp.sin(x) * y\n",
    "\n",
    "f.defjvps(lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y,\n",
    "          lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot)\n",
    "# 这里 x_dot 是切向量的分量，y_dot 是切向量的分量\n",
    "# primal_out 是原函数的输出\n",
    "# x, y 是原函数的输入\n",
    "\n",
    "# 这个 defjvps 的用法是：\n",
    "# “如果你要对 f(x, y) 在 x 上求导，就用第一个函数；\n",
    "# 如果你要在 y 上求导，就用第二个函数。”\n",
    "\n",
    "\n",
    "print(f(2., 3.))\n",
    "y, y_dot = jvp(f, (2., 3.), (1., 0.))\n",
    "print(y)\n",
    "print(y_dot)\n",
    "print(grad(f)(2., 3.))"
   ],
   "id": "35d94521c6217718",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import custom_vjp\n",
    "\n",
    "@custom_vjp\n",
    "def f(x, y):\n",
    "  return jnp.sin(x) * y\n",
    "\n",
    "def f_fwd(x, y):\n",
    "# Returns primal output and residuals to be used in backward pass by `f_bwd`.\n",
    "  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n",
    "# residual 是可以在反向传播中使用的中间变量\n",
    "# residual 可以是自行决定的\n",
    "\n",
    "def f_bwd(res, g):\n",
    "  cos_x, sin_x, y = res # Gets residuals computed in `f_fwd`\n",
    "  return (cos_x * g * y, sin_x * g)\n",
    "\n",
    "f.defvjp(f_fwd, f_bwd)\n",
    "\n",
    "print(grad(f)(2., 3.))"
   ],
   "id": "6649e19789cdd4cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Example problems",
   "id": "6c95732fea18156d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 重新定义求导规则解决数值稳定性的问题\n",
    "\n",
    "def log1pexp(x):\n",
    "  return jnp.log(1. + jnp.exp(x))\n",
    "\n",
    "@custom_jvp\n",
    "def log1pexp(x):\n",
    "  return jnp.log(1. + jnp.exp(x))\n",
    "\n",
    "@log1pexp.defjvp\n",
    "def log1pexp_jvp(primals, tangents):\n",
    "  x, = primals\n",
    "  x_dot, = tangents\n",
    "  ans = log1pexp(x)\n",
    "  ans_dot = (1 - 1/(1 + jnp.exp(x))) * x_dot\n",
    "  return ans, ans_dot\n",
    "\n",
    "print(grad(log1pexp)(100.))"
   ],
   "id": "fadfabc2828782e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 或者换一种写法\n",
    "@custom_jvp\n",
    "def log1pexp(x):\n",
    "  return jnp.log(1. + jnp.exp(x))\n",
    "\n",
    "log1pexp.defjvps(lambda t, ans, x: (1 - 1/(1 + jnp.exp(x))) * t)\n",
    "# t 是切向量的分量 只有一个分量\n",
    "# ans 是原函数的输出\n",
    "# x 是原函数的输入\n",
    "\n",
    "jvp(log1pexp, (8.,), (1.,))"
   ],
   "id": "72d63bdd8fd42055",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 重新定义求导规则解决函数奇点\n",
    "\n",
    "@custom_jvp\n",
    "def f(x):\n",
    "  return x / (1 + jnp.sqrt(x))\n",
    "\n",
    "@f.defjvp\n",
    "def f_jvp(primals, tangents):\n",
    "  x, = primals\n",
    "  x_dot, = tangents\n",
    "  ans = f(x)\n",
    "  ans_dot = ((jnp.sqrt(x) + 2) / (2 * (jnp.sqrt(x) + 1)**2)) * x_dot\n",
    "  return ans, ans_dot\n",
    "\n",
    "print(grad(f)(0.))"
   ],
   "id": "d7967361c9fe521a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 或者换一种写法\n",
    "@custom_jvp\n",
    "def f(x):\n",
    "  return x / (1 + jnp.sqrt(x))\n",
    "\n",
    "f.defjvps(lambda t, ans, x: ((jnp.sqrt(x) + 2) / (2 * (jnp.sqrt(x) + 1)**2)) * t)\n",
    "\n",
    "print(grad(f)(0.))"
   ],
   "id": "d28a2fd08a39652c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "@custom_vjp\n",
    "def clip_gradient(lo, hi, x):\n",
    "  return x  # identity function\n",
    "\n",
    "def clip_gradient_fwd(lo, hi, x):\n",
    "  return x, (lo, hi)  # save bounds as residuals\n",
    "\n",
    "def clip_gradient_bwd(res, g):\n",
    "  lo, hi = res\n",
    "  return (None, None, jnp.clip(g, lo, hi))  # use None to indicate zero cotangents for lo and hi\n",
    "# None 的意思是对 lo 和 hi 没有导数\n",
    "\n",
    "clip_gradient.defvjp(clip_gradient_fwd, clip_gradient_bwd)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = jnp.linspace(0, 10, 1000)\n",
    "\n",
    "plt.plot(jnp.sin(t))\n",
    "plt.plot(vmap(grad(jnp.sin))(t))"
   ],
   "id": "4c7d5609d2d4835f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def clip_sin(x):\n",
    "  x = clip_gradient(-0.75, 0.75, x)\n",
    "  return jnp.sin(x)\n",
    "# 这是一个复合函数其实，所以 g 参数就是 cosx\n",
    "\n",
    "plt.plot(clip_sin(t))\n",
    "plt.plot(vmap(grad(clip_sin))(t))"
   ],
   "id": "4e5153c305c96a47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Basic usage of jax.custom_jvp and jax.custom_vjp APIs",
   "id": "90d3238cab0ddc44"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "摘抄：\n",
    "\n",
    "Even though we defined only a JVP rule and no VJP rule, we can use both forward- and reverse-mode differentiation on f. JAX will automatically transpose the linear computation on tangent values from our custom JVP rule, computing the VJP as efficiently as if we had written the rule by hand.\n",
    "\n",
    "jax.custom_jvp() suffices for controlling both forward- and, via JAX’s automatic transposition, reverse-mode differentiation behavior.\n",
    "\n",
    "As with jax.custom_jvp(), the custom VJP rule composed of f_fwd and f_bwd is not invoked if differentiation is not applied.\n",
    "\n",
    "Forward-mode autodiff cannot be used on the jax.custom_vjp() function and will raise an error:"
   ],
   "id": "c2165f18f5fbc26d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# You can use Python control flow with jax.custom_jvp():\n",
    "# But we cannot use JIT with the following function:\n",
    "\n",
    "@custom_jvp\n",
    "def f(x):\n",
    "  if x > 0:\n",
    "    return jnp.sin(x)\n",
    "  else:\n",
    "    return jnp.cos(x)\n",
    "\n",
    "@f.defjvp\n",
    "def f_jvp(primals, tangents):\n",
    "  x, = primals\n",
    "  x_dot, = tangents\n",
    "  ans = f(x)\n",
    "  if x > 0:\n",
    "    return ans, 2 * x_dot\n",
    "  else:\n",
    "    return ans, 3 * x_dot\n"
   ],
   "id": "4bec3d8d73106e17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## More features and details",
   "id": "a6494b41b3cbb1f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Working with list / tuple / dict containers (and other pytrees)",
   "id": "b4ee1ae8cc0c2179"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import namedtuple\n",
    "Point = namedtuple(\"Point\", [\"x\", \"y\"])\n",
    "\n",
    "@custom_jvp\n",
    "def f(pt):\n",
    "  x, y = pt.x, pt.y\n",
    "  return {'a': x ** 2,\n",
    "          'b': (jnp.sin(x), jnp.cos(y))}\n",
    "\n",
    "@f.defjvp\n",
    "def f_jvp(primals, tangents):\n",
    "  pt, = primals # 这儿的逗号不能省略\n",
    "  pt_dot, =  tangents\n",
    "  ans = f(pt)\n",
    "  ans_dot = {'a': 2 * pt.x * pt_dot.x,\n",
    "             'b': (jnp.cos(pt.x) * pt_dot.x, -jnp.sin(pt.y) * pt_dot.y)}\n",
    "  return ans, ans_dot\n",
    "\n",
    "def fun(pt):\n",
    "  dct = f(pt)\n",
    "  return dct['a'] + dct['b'][0]\n",
    "\n",
    "pt = Point(1., 2.)\n",
    "#print(f(pt))\n",
    "print(grad(fun)(pt))"
   ],
   "id": "bdc2c65e0f0e55d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@custom_vjp\n",
    "def f(pt):\n",
    "  x, y = pt.x, pt.y\n",
    "  return {'a': x ** 2,\n",
    "          'b': (jnp.sin(x), jnp.cos(y))}\n",
    "\n",
    "def f_fwd(pt):\n",
    "  return f(pt), pt\n",
    "\n",
    "def f_bwd(pt, g):\n",
    "  a_bar, (b0_bar, b1_bar) = g['a'], g['b']\n",
    "  x_bar = 2 * pt.x * a_bar + jnp.cos(pt.x) * b0_bar\n",
    "  y_bar = -jnp.sin(pt.y) * b1_bar\n",
    "  return (Point(x_bar, y_bar),)\n",
    "\n",
    "f.defvjp(f_fwd, f_bwd)\n",
    "\n",
    "def fun(pt):\n",
    "  dct = f(pt)\n",
    "  return dct['a'] + dct['b'][0]\n",
    "\n",
    "pt = Point(1., 2.)\n",
    "#print(f(pt))\n",
    "print(grad(fun)(pt))"
   ],
   "id": "a9e18dcd9b46ccf5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Handling non-differentiable arguments",
   "id": "ae179682be1a5ec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(custom_jvp, nondiff_argnums=(0,)) #不加这个会报错 AttributeError: 'function' object has no attribute 'defjvp'\n",
    "def app(f, x):\n",
    "  return f(x)\n",
    "\n",
    "@app.defjvp\n",
    "def app_jvp(f, primals, tangents):\n",
    "  x, = primals\n",
    "  x_dot, = tangents\n",
    "  return f(x), 2. * x_dot\n",
    "\n",
    "\n",
    "print(app(lambda x: x ** 3, 3.))\n",
    "print(grad(app, 1)(lambda x: x ** 3, 3.))"
   ],
   "id": "acdbede6aeec66c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Notice the gotcha here: no matter where in the argument list these parameters appear, they’re placed at the start of the signature of the corresponding JVP rule. Here’s another example:",
   "id": "e5fed6142a45679d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@partial(custom_jvp, nondiff_argnums=(0, 2))\n",
    "def app2(f, x, g):\n",
    "  return f(g((x)))\n",
    "\n",
    "@app2.defjvp\n",
    "def app2_jvp(f, g, primals, tangents):\n",
    "  x, = primals\n",
    "  x_dot, = tangents\n",
    "  return f(g(x)), 3. * x_dot\n",
    "\n",
    "print(app2(lambda x: x ** 3, 3., lambda y: 5 * y))\n",
    "print(grad(app2, 1)(lambda x: x ** 3, 3., lambda y: 5 * y))"
   ],
   "id": "48b62808876ffbf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "@partial(custom_vjp, nondiff_argnums=(0,))\n",
    "def app(f, x):\n",
    "  return f(x)\n",
    "\n",
    "def app_fwd(f, x):\n",
    "  return f(x), x\n",
    "\n",
    "def app_bwd(f, x, g):\n",
    "  return (5 * g,)\n",
    "\n",
    "app.defvjp(app_fwd, app_bwd)\n",
    "\n",
    "print(app(lambda x: x ** 2, 4.))\n",
    "print(grad(app, 1)(lambda x: x ** 2, 4.))"
   ],
   "id": "e09cb36bebdc7a01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "You don’t need to use nondiff_argnums with array-valued arguments, such as, for example, ones with the integer dtype. Instead, nondiff_argnums should only be used for argument values that don’t correspond to JAX types (essentially don’t correspond to array types), like Python callables or strings. If JAX detects that an argument indicated by nondiff_argnums contains a JAX Tracer, then an error is raised. The clip_gradient function above is a good example of not using nondiff_argnums for integer-dtype array arguments.",
   "id": "94fac04275919e8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 我试过了 把上面代码的第一行改成下面这样就会报错\n",
    "# @partial(custom_vjp, nondiff_argnums=(0,1))"
   ],
   "id": "a42e69ee90358b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
