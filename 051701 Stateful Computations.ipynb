{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 如果不用 JIT，计数器的写法很简单\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "class Counter:\n",
    "  \"\"\"A simple counter.\"\"\"\n",
    "\n",
    "  def __init__(self):\n",
    "    self.n = 0\n",
    "\n",
    "  def count(self) -> int:\n",
    "    \"\"\"Increments the counter and returns the new value.\"\"\"\n",
    "    self.n += 1\n",
    "    return self.n\n",
    "\n",
    "  def reset(self):\n",
    "    \"\"\"Resets the counter to zero.\"\"\"\n",
    "    self.n = 0\n",
    "\n",
    "\n",
    "counter = Counter()\n",
    "\n",
    "for _ in range(3):\n",
    "  print(counter.count())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 但是，如果使用 JIT，就会出现错误\n",
    "# 这是因为 JIT 的函数不能有副作用，不能修改全局变量的状态\n",
    "\n",
    "counter.reset()\n",
    "fast_count = jax.jit(counter.count)\n",
    "\n",
    "for _ in range(3):\n",
    "  print(fast_count())\n",
    "\n",
    "counter.reset()\n",
    "for _ in range(3):\n",
    "  print(fast_count())"
   ],
   "id": "77c2c6f7f88a548b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 我们可以将计数器的状态作为参数传递给函数再返回\n",
    "# 这样就可以避免副作用的问题\n",
    "\n",
    "CounterState = int\n",
    "\n",
    "class CounterV2:\n",
    "\n",
    "  def count(self, n: CounterState) -> tuple[int, CounterState]:\n",
    "    # You could just return n+1, but here we separate its role as\n",
    "    # the output and as the counter state for didactic purposes.\n",
    "    return n+1, n+1\n",
    "\n",
    "  def reset(self) -> CounterState:\n",
    "    return 0\n",
    "\n",
    "counter = CounterV2()\n",
    "state = counter.reset()\n",
    "\n",
    "for _ in range(3):\n",
    "  value, state = counter.count(state)\n",
    "  print(value)\n",
    "\n",
    "state = counter.reset()\n",
    "fast_count = jax.jit(counter.count)\n",
    "\n",
    "for _ in range(3):\n",
    "  value, state = fast_count(state)\n",
    "  print(value)"
   ],
   "id": "cde91e72083dcb27",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num = 0\n",
    "\n",
    "def test(num):\n",
    "  return num + 1\n",
    "num = test(num)\n",
    "print(num)\n",
    "\n",
    "print(\"Now we will use JAX to compile the function.\")\n",
    "fast_test = jax.jit(test)\n",
    "num = fast_test(num)\n",
    "print(num)\n",
    "num = fast_test(num)\n",
    "print(num)\n",
    "num = fast_test(num)\n",
    "print(num)\n",
    "num = fast_test(num)\n",
    "print(num)"
   ],
   "id": "4f6a313f1207cee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 关于 python 中变量\n",
    "# 这样，理解函数的值传递和引用传递就简单了\n",
    "# 实际上既不是参数传递和对象赋值是一样的原理\n",
    "# Python 中，重新赋值实际上是重新创建了一个同名对象，但是id会改变，这对所有类型的变量都成立。\n",
    "# 而对于可变对象，若通过支持的方法对对象做些修改，对象的id不改变。\n",
    "\n",
    "print(\"第一个例子\")\n",
    "x = 0\n",
    "print(id(x))\n",
    "x = 0\n",
    "print(id(x))\n",
    "\n",
    "print(\"第二个例子\")\n",
    "x= [0,1,2]\n",
    "print(id(x))\n",
    "x = [0,1,2]\n",
    "print(id(x))\n",
    "\n",
    "print(\"第三个例子\")\n",
    "x = [0,1,2]\n",
    "print(id(x))\n",
    "x[0] = 1\n",
    "print(id(x))\n",
    "x.append(3)\n",
    "print(id(x))\n",
    "\n",
    "a = 100\n",
    "print(f\"a的变量id是{id(a)}.\")\n",
    "def func(x, a):\n",
    "    a +=1\n",
    "    print(f\"a的变量id是{id(a)}.\")\n",
    "    print(id(x))\n",
    "    x[0] = -1\n",
    "    print(id(x))\n",
    "    x.append(4)\n",
    "    print(x)\n",
    "func(x,a)"
   ],
   "id": "ce1909d0406ac835",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 一个例子：线性回归",
   "id": "93903aaddc08c5dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import NamedTuple\n",
    "\n",
    "class Params(NamedTuple):\n",
    "  weight: jnp.ndarray\n",
    "  bias: jnp.ndarray\n",
    "\n",
    "\n",
    "def init(rng) -> Params:\n",
    "  \"\"\"Returns the initial model params.\"\"\"\n",
    "  weights_key, bias_key = jax.random.split(rng)\n",
    "  weight = jax.random.normal(weights_key, ())\n",
    "  bias = jax.random.normal(bias_key, ())\n",
    "  return Params(weight, bias)\n",
    "\n",
    "\n",
    "def loss(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
    "  \"\"\"Computes the least squares error of the model's predictions on x against y.\"\"\"\n",
    "  pred = params.weight * x + params.bias\n",
    "  return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "@jax.jit\n",
    "def update(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> Params:\n",
    "  \"\"\"Performs one SGD update step on params using the given data.\"\"\"\n",
    "  grad = jax.grad(loss)(params, x, y)\n",
    "\n",
    "  # If we were using Adam or another stateful optimizer,\n",
    "  # we would also do something like\n",
    "  #\n",
    "  #   updates, new_optimizer_state = optimizer(grad, optimizer_state)\n",
    "  #\n",
    "  # and then use `updates` instead of `grad` to actually update the params.\n",
    "  # (And we'd include `new_optimizer_state` in the output, naturally.)\n",
    "\n",
    "  new_params = jax.tree.map(\n",
    "      lambda param, g: param - g * LEARNING_RATE, params, grad)\n",
    "\n",
    "  return new_params"
   ],
   "id": "dda37ec25a6c83e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = jax.random.key(42)\n",
    "\n",
    "# Generate true data from y = w*x + b + noise\n",
    "true_w, true_b = 2, -1\n",
    "x_rng, noise_rng = jax.random.split(rng)\n",
    "xs = jax.random.normal(x_rng, (128, 1))\n",
    "noise = jax.random.normal(noise_rng, (128, 1)) * 0.2\n",
    "ys = xs * true_w + true_b + noise\n",
    "\n",
    "# Fit regression\n",
    "params = init(rng)\n",
    "for _ in range(1000):\n",
    "  params = update(params, xs, ys)\n",
    "\n",
    "plt.scatter(xs, ys)\n",
    "plt.plot(xs, params.weight * xs + params.bias, c='red', label='Model Prediction')\n",
    "plt.legend();"
   ],
   "id": "d6ff528b8c4aaa99",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
