{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random\n",
    "\n",
    "key = random.key(0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 预热：计算二阶导数\n",
    "\n",
    "def hessian(f):\n",
    "  return jax.jacfwd(jax.grad(f))\n",
    "\n",
    "def f(x):\n",
    "  return jnp.dot(x, x)\n",
    "\n",
    "hessian(f)(jnp.array([1., 2., 3.]))"
   ],
   "id": "ab58520a982f8636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Higher-order optimization",
   "id": "fbe5f718de5e30f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Stopping gradients",
   "id": "2720df1c8fba9a9a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Value function and initial parameters\n",
    "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
    "theta = jnp.array([0.1, -0.1, 0.])\n",
    "\n",
    "# An example transition.\n",
    "s_tm1 = jnp.array([1., 2., -1.])\n",
    "r_t = jnp.array(1.)\n",
    "s_t = jnp.array([2., 1., 0.])"
   ],
   "id": "d875436eac35ddf1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def td_loss(theta, s_tm1, r_t, s_t):\n",
    "  v_tm1 = value_fn(theta, s_tm1)\n",
    "  target = r_t + value_fn(theta, s_t)\n",
    "  return -0.5 * ((target - v_tm1) ** 2)\n",
    "\n",
    "td_update = jax.grad(td_loss)\n",
    "delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
    "\n",
    "delta_theta"
   ],
   "id": "9936d44482ba10e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def td_loss(theta, s_tm1, r_t, s_t):\n",
    "  v_tm1 = value_fn(theta, s_tm1)\n",
    "  target = r_t + value_fn(theta, s_t)\n",
    "  return -0.5 * ((jax.lax.stop_gradient(target) - v_tm1) ** 2)\n",
    "# This is the most important line in the code.\n",
    "\n",
    "td_update = jax.grad(td_loss)\n",
    "delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
    "\n",
    "print(delta_theta)\n",
    "\n",
    "# 手动写代码验证上面的结果\n",
    "grad_fn = jax.grad(value_fn)\n",
    "(r_t+value_fn(theta, s_t)-value_fn(theta, s_tm1))*grad_fn(theta, s_tm1)"
   ],
   "id": "b74ba0bb6c1c403f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Straight-through estimator using stop_gradient",
   "id": "6b19864bca53767d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def f(x):\n",
    "  return jnp.round(x)  # non-differentiable\n",
    "\n",
    "def straight_through_f(x):\n",
    "  # Create an exactly-zero expression with Sterbenz lemma that has\n",
    "  # an exactly-one gradient.\n",
    "  zero = x - jax.lax.stop_gradient(x)\n",
    "  return zero + jax.lax.stop_gradient(f(x))\n",
    "\n",
    "print(\"f(x): \", f(3.2))\n",
    "print(\"straight_through_f(x):\", straight_through_f(3.2))\n",
    "\n",
    "print(\"grad(f)(x):\", jax.grad(f)(3.2))\n",
    "print(\"grad(straight_through_f)(x):\", jax.grad(straight_through_f)(3.2))\n",
    "\n",
    "# 前向传播就是函数求值\n",
    "# 反向传播就是函数求导（用链式法则）"
   ],
   "id": "e994785c9148900a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Per-example gradients",
   "id": "5b0046d0eb4a0603"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "perex_grads = jax.jit(jax.vmap(jax.grad(td_loss), in_axes=(None, 0, 0, 0)))\n",
    "\n",
    "# Test it:\n",
    "batched_s_tm1 = jnp.stack([s_tm1, s_tm1])\n",
    "batched_r_t = jnp.stack([r_t, r_t])\n",
    "batched_s_t = jnp.stack([s_t, s_t])\n",
    "\n",
    "perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t)"
   ],
   "id": "1621a4ff12c82f1c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hessian-vector products with jax.grad-of-jax.grad",
   "id": "c778fad8612e7930"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def hvp(f, x, v):\n",
    "    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
    "\n",
    "def gprod(f, x, v):\n",
    "    return grad(f)(x) @ v\n",
    "\n",
    "# 或者不用 lambda 用下面的这种写法？\n",
    "def hvp2(f, x, v):\n",
    "    return grad(gprod, argnums=1)(x) @ v"
   ],
   "id": "d446b7bf6deffa0b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# jax.grad 是什么形状的？\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "# 定义目标函数\n",
    "def f(x):\n",
    "    return jnp.sum(x ** 2)  # 简单的二次函数\n",
    "\n",
    "# 输入变量\n",
    "x = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "# 计算梯度\n",
    "grad_f = grad(f)(x)\n",
    "\n",
    "print(\"输入变量 x 的形状:\", x.shape)\n",
    "print(\"梯度向量 grad_f 的形状:\", grad_f.shape)"
   ],
   "id": "a5b372820312c2fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 补课：点积和矩阵乘法\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# 一维向量点积（标量）\n",
    "a = jnp.array([1, 2, 3])\n",
    "b = jnp.array([4, 5, 6])\n",
    "print(\"一维向量点积（标量）\")\n",
    "print(jnp.dot(a, b))   # 32\n",
    "print(a @ b)    # 32\n",
    "print(jnp.matmul(a, b))  # [4 10 18]\n",
    "\n",
    "print(jnp.inner(a, b))  # 32\n",
    "print(jnp.vdot(a, b))   # 32\n",
    "\n",
    "# 二维矩阵乘法\n",
    "A = jnp.array([[1, 2], [3, 4]])\n",
    "B = jnp.array([[5, 6], [7, 8]])\n",
    "print(jnp.dot(A, B))   # works\n",
    "print(A @ B)           # works\n",
    "print(jnp.matmul(A, B))   # works\n",
    "\n",
    "# 更高维度建议用 jnp.matmul 或 @\n",
    "# 不推荐用 jnp.dot\n",
    "\n",
    "# jnp.tensordot 这个比较复杂，有待进一步学习"
   ],
   "id": "c1061b41522af656",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Jacobians and Hessians using jax.jacfwd and jax.jacrev",
   "id": "def00cb5ccbb0616"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import jacfwd, jacrev\n",
    "\n",
    "# Define a sigmoid function.\n",
    "def sigmoid(x):\n",
    "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
    "\n",
    "# Outputs probability of a label being true.\n",
    "def predict(W, b, inputs):\n",
    "    return sigmoid(jnp.dot(inputs, W) + b)\n",
    "\n",
    "# Build a toy dataset.\n",
    "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
    "                   [0.88, -1.08, 0.15],\n",
    "                   [0.52, 0.06, -1.30],\n",
    "                   [0.74, -2.49, 1.39]])\n",
    "\n",
    "# Initialize random model coefficients\n",
    "key, W_key, b_key = random.split(key, 3)\n",
    "W = random.normal(W_key, (3,))\n",
    "b = random.normal(b_key, ())\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "J = jacfwd(f)(W)\n",
    "print(\"jacfwd result, with shape\", J.shape)\n",
    "print(J)\n",
    "\n",
    "J = jacrev(f)(W)\n",
    "print(\"jacrev result, with shape\", J.shape)\n",
    "print(J)"
   ],
   "id": "3cc778fb83ba272f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These two functions compute the same values (up to machine numerics), but differ in their implementation: jax.jacfwd() uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices (more outputs than inputs), while jax.jacrev() uses reverse-mode, which is more efficient for “wide” Jacobian matrices (more inputs than outputs). For matrices that are near-square, jax.jacfwd() probably has an edge over jax.jacrev().",
   "id": "6481dfa457b9688e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def predict_dict(params, inputs):\n",
    "    return predict(params['W'], params['b'], inputs)\n",
    "\n",
    "J_dict_fwd = jacfwd(predict_dict)({'W': W, 'b': b}, inputs)\n",
    "J_dict_rev = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n",
    "\n",
    "for k, v in J_dict_fwd.items():\n",
    "    print(f\"Jacobian from {k} to logits is\")\n",
    "    print(v)\n",
    "for k, v in J_dict_rev.items():\n",
    "    print(f\"Jacobian from {k} to logits is\")\n",
    "    print(v)"
   ],
   "id": "ac2dfdbb8bc679f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# How it’s made: Two foundational autodiff functions",
   "id": "a23ed7af13655e3a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Jacobian-Vector products (JVPs, a.k.a. forward-mode autodiff)",
   "id": "50c6dfad756e9cfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import jvp\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "v = random.normal(subkey, W.shape)\n",
    "\n",
    "# Push forward the vector `v` along `f` evaluated at `W`\n",
    "y, u = jvp(f, (W,), (v,))\n",
    "print(\"使用 jvp 计算的结果\")\n",
    "print(\"y:\", y)\n",
    "print(\"u:\", u)\n",
    "\n",
    "print(\"下面是手动验证的代码\")\n",
    "print(f\"y: {f(W)}\\nu: {jax.jacobian(f)(W)@v}\")\n",
    "\n",
    "# 教程中的 “FLOP” 是 Floating Point Operation（浮点运算） 的缩写，是衡量计算量的一个基本单位。"
   ],
   "id": "529c7006cf2b3a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vector-Jacobian products (VJPs, a.k.a. reverse-mode autodiff)",
   "id": "d915ed078ccadcb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from jax import vjp\n",
    "\n",
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "y, vjp_fun = vjp(f, W)\n",
    "\n",
    "key, subkey = random.split(key)\n",
    "#u = random.normal(subkey, y.shape)\n",
    "u = jnp.array([1., 0., 0., 0.])  # 只取第一个元素的梯度\n",
    "\n",
    "# Pull back the covector `u` along `f` evaluated at `W`\n",
    "v = vjp_fun(u)\n",
    "\n",
    "u, v"
   ],
   "id": "5f8fb7e54d60dedf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Vector-valued gradients with VJPs",
   "id": "d55d63f346829e15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def vgrad(f, x):\n",
    "  y, vjp_fn = vjp(f, x)\n",
    "  return vjp_fn(jnp.ones(y.shape))[0]\n",
    "\n",
    "x = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "\n",
    "print(vgrad(lambda x: 3*x**2, x))"
   ],
   "id": "71dd161d776cf612",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 或者也可以手动实现\n",
    "\n",
    "f = lambda x:3*x**2\n",
    "vdf = jax.vmap(grad(f))\n",
    "vdf = vdf(x.reshape(-1))\n",
    "print(vdf, \"但是这个形状变了！\")\n",
    "\n",
    "varf = lambda x: jnp.sum(3 * x ** 2)\n",
    "print(varf(x), \"这个函数变了！\")\n",
    "vardf = jax.grad(varf)\n",
    "print(vardf(x))  # 输出 shape 为 (2,2)，结果是全 6"
   ],
   "id": "62f7846e1417e5c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Hessian-vector products using both forward- and reverse-mode",
   "id": "88407a1365e9935"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def f(X):\n",
    "  return jnp.sum(jnp.tanh(X)**2)\n",
    "\n",
    "key, subkey1, subkey2 = random.split(key, 3)\n",
    "X = random.normal(subkey1, (30, 40))\n",
    "V = random.normal(subkey2, (30, 40))\n",
    "\n",
    "# forward-over-reverse\n",
    "def hvp(f, primals, tangents):\n",
    "  return jvp(grad(f), primals, tangents)[1]\n",
    "\n",
    "# Reverse-over-forward\n",
    "def hvp_revfwd(f, primals, tangents):\n",
    "  g = lambda primals: jvp(f, primals, tangents)[1]\n",
    "  return grad(g)(primals)\n",
    "\n",
    "# Reverse-over-reverse, only works for single arguments\n",
    "def hvp_revrev(f, primals, tangents):\n",
    "  x, = primals\n",
    "  v, = tangents\n",
    "  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
    "\n",
    "print(\"Forward over reverse\")\n",
    "%timeit -n10 -r3 hvp(f, (X,), (V,))\n",
    "print(\"Reverse over forward\")\n",
    "%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))\n",
    "print(\"Reverse over reverse\")\n",
    "%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))\n",
    "print(\"Naive full Hessian materialization\")\n",
    "%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)"
   ],
   "id": "2b7d115173b37ce1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Composing VJPs, JVPs, and jax.vmap",
   "id": "716126a4544f68eb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Jacobian-Matrix and Matrix-Jacobian products",
   "id": "4880b054a351884f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T01:26:29.548731Z",
     "start_time": "2025-05-19T01:26:27.404022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Isolate the function from the weight matrix to the predictions\n",
    "f = lambda W: predict(W, b, inputs)\n",
    "\n",
    "# Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.\n",
    "# First, use a list comprehension to loop over rows in the matrix M.\n",
    "def loop_mjp(f, x, M):\n",
    "    y, vjp_fun = vjp(f, x)\n",
    "    return jnp.vstack([vjp_fun(mi) for mi in M])\n",
    "\n",
    "# Now, use vmap to build a computation that does a single fast matrix-matrix\n",
    "# multiply, rather than an outer loop over vector-matrix multiplies.\n",
    "def vmap_mjp(f, x, M):\n",
    "    y, vjp_fun = vjp(f, x)\n",
    "    outs, = vmap(vjp_fun)(M)\n",
    "    return outs\n",
    "\n",
    "def vmap_mjp2(f, x, M):\n",
    "    y, jvp_fun = vmap(jvp, in_axes=(None, None, 0))(f, (x,), (M,))\n",
    "    outs = jvp_fun\n",
    "    return outs\n",
    "\n",
    "key = random.key(0)\n",
    "num_covecs = 128\n",
    "U = random.normal(key, (num_covecs,) + y.shape)\n",
    "V = random.normal(key, (num_covecs,) + W.shape)\n",
    "\n",
    "\n",
    "loop_vs = loop_mjp(f, W, M=U)\n",
    "print('Non-vmapped Matrix-Jacobian product')\n",
    "%timeit -n10 -r3 loop_mjp(f, W, M=U)\n",
    "\n",
    "print('\\nVmapped Matrix-Jacobian product, reverse-mode')\n",
    "vmap_vs = vmap_mjp(f, W, M=U)\n",
    "%timeit -n10 -r3 vmap_mjp(f, W, M=U)\n",
    "\n",
    "print('\\nVmapped Matrix-Jacobian product forward-mode')\n",
    "vmap_vs2 = vmap_mjp2(f, W, M=V)\n",
    "%timeit -n10 -r3 vmap_mjp2(f, W, M=V)\n",
    "\n",
    "# assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical'"
   ],
   "id": "d2be2fd054ff571",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\weiz828\\AppData\\Local\\Temp\\ipykernel_26724\\2257824564.py:8: DeprecationWarning: vstack requires ndarray or scalar arguments, got <class 'tuple'> at position 0. In a future JAX release this will be an error.\n",
      "  return jnp.vstack([vjp_fun(mi) for mi in M])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-vmapped Matrix-Jacobian product\n",
      "63.4 ms ± 5.27 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "\n",
      "Vmapped Matrix-Jacobian product, reverse-mode\n",
      "2.65 ms ± 187 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n",
      "\n",
      "Vmapped Matrix-Jacobian product forward-mode\n",
      "1.49 ms ± 68.4 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## The implementation of jax.jacfwd and jax.jacrev",
   "id": "e5581e737b0ffcf3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T01:24:00.126962Z",
     "start_time": "2025-05-19T01:24:00.094829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax import jacrev as builtin_jacrev\n",
    "\n",
    "def our_jacrev(f):\n",
    "    def jacfun(x):\n",
    "        y, vjp_fun = vjp(f, x)\n",
    "        # Use vmap to do a matrix-Jacobian product.\n",
    "        # Here, the matrix is the Euclidean basis, so we get all\n",
    "        # entries in the Jacobian at once.\n",
    "        J, = vmap(vjp_fun, in_axes=0)(jnp.eye(len(y)))\n",
    "        return J\n",
    "    return jacfun\n",
    "\n",
    "assert jnp.allclose(builtin_jacrev(f)(W), our_jacrev(f)(W)), 'Incorrect reverse-mode Jacobian results!'"
   ],
   "id": "5e7a7c85a228dd53",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T01:25:13.549521Z",
     "start_time": "2025-05-19T01:25:13.478761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from jax import jacfwd as builtin_jacfwd\n",
    "\n",
    "def our_jacfwd(f):\n",
    "    def jacfun(x):\n",
    "        _jvp = lambda s: jvp(f, (x,), (s,))[1]\n",
    "        Jt = vmap(_jvp, in_axes=1)(jnp.eye(len(x)))\n",
    "        return jnp.transpose(Jt)\n",
    "    return jacfun\n",
    "\n",
    "assert jnp.allclose(builtin_jacfwd(f)(W), our_jacfwd(f)(W)), 'Incorrect forward-mode Jacobian results!'"
   ],
   "id": "636575db07ef9660",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T01:32:31.116543Z",
     "start_time": "2025-05-19T01:32:31.099461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 补课：闭包与函数嵌套调用\n",
    "\n",
    "def func1(a):\n",
    "    def func2(b):\n",
    "        return a + b\n",
    "    return func2\n",
    "\n",
    "func1(1)(2)  # 3"
   ],
   "id": "71a06fc636c0a358",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
